{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saskinosie/CalibrateAI-5-12-25/blob/main/_calibrateai__hack_day_micro_conference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Semantic Search with Weaviate's Query Agent, LlamaIndex and Comet\n",
        "\n",
        "##Pre-requisites\n",
        "For this workshop you will need:\n",
        "\n",
        "* A (free) Weaviate Cloud (WCD) account\n",
        "* A cluster set up in WCD\n",
        "* The REST endpoint for your cluster\n",
        "* Your cluster Admin API key\n",
        "* An OpenAI API key\n",
        "\n",
        "In this workshop we will create a Retrieval Augmented Generation system leveraging Weaviate's Query Agent, LlamaIndex and Comet.\n",
        "\n",
        "\n",
        "We’re utilizing LlamaIndex to transform full-text PDF research articles into manageable, structured text chunks. These chunks are enhanced with metadata and section detection logic, then uploaded into a Weaviate vector database to support semantic search over a collection of space medicine literature allowing us to query our data using natural languge using Weaviate's query agent. In the subsequent woerkshop, we will use Comet's end-to-end model evaluation platform to benchmark our RAG system.\n",
        "\n",
        "### Link to slide deck\n",
        "https://docs.google.com/presentation/d/1UbDpA0dhuHiiSu5vsiv_PPA9HzgzZKcRn7t4zuoFzzA/edit?usp=sharing\n",
        "\n",
        "### Link to repo and instructions for copllecting REST endpoint and API keys\n",
        "https://github.com/saskinosie/CalibrateAI-5-12-25\n",
        "\n",
        "### Link to G-Drive folder with research articles https://drive.google.com/drive/folders/18iu8lGJ0SEZcISkUqc20pecGrb61Mo7s?usp=drive_link"
      ],
      "metadata": {
        "id": "jveaX8o_L8xq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWxTOKVNAc4k"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index pymupdf weaviate-client weaviate-agents\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import json\n",
        "import re\n",
        "import requests\n",
        "from llama_index.core import Document\n",
        "from llama_index.core.node_parser import HierarchicalNodeParser\n",
        "import weaviate"
      ],
      "metadata": {
        "id": "mxgSh8ZkFTcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "WEAVIATE_URL = userdata.get(\"WEAVIATE_URL\")\n",
        "WEAVIATE_API_KEY = userdata.get(\"WEAVIATE_API_KEY\")\n",
        "OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "print(\"Weaviate URL:\", WEAVIATE_URL)\n",
        "print(\"Weaviate API Key:\", WEAVIATE_API_KEY)\n"
      ],
      "metadata": {
        "id": "MomG5MNPAh85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = weaviate.connect_to_weaviate_cloud(\n",
        "    WEAVIATE_URL,\n",
        "    auth_credentials=weaviate.AuthApiKey(WEAVIATE_API_KEY),\n",
        "    headers={\n",
        "        \"X-OpenAI-Api-Key\": OPENAI_API_KEY\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "n0Mh5vLSFApA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert client.is_ready(), \"Weaviate client is not ready. Check credentials and endpoint.\"\n"
      ],
      "metadata": {
        "id": "KhG_pHeeGw7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client.is_ready()"
      ],
      "metadata": {
        "id": "IHgm4vnhG0F6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from weaviate.classes.config import Configure\n",
        "client.collections.create(\n",
        "    name = \"SpaceMedResearch\",\n",
        "    vectorizer_config= [\n",
        "            Configure.NamedVectors.text2vec_weaviate(\n",
        "                name=\"main_vector\",\n",
        "                model=\"Snowflake/snowflake-arctic-embed-l-v2.0\",\n",
        "                source_properties=[\"title\", \"content\"],\n",
        "            )\n",
        "        ],\n",
        "    )\n"
      ],
      "metadata": {
        "id": "iDUSO2_0Inr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdrive_links = [\n",
        "    \"https://drive.google.com/file/d/1bNX5nZTif8roMK1bFaJmHF6wxapi5YDg/view?usp=sharing\",\n",
        "    \"https://drive.google.com/file/d/1FZkvMOyTP_-kSIyx9VewaV9tP_XwpZXK/view?usp=drive_link\",\n",
        "    \"https://drive.google.com/file/d/1jfcCLHmAazvs7DnAhd3jS0LOb7qOctMc/view?usp=drive_link\",\n",
        "    \"https://drive.google.com/file/d/1K8D6VOe2aAX6tIfJWzF2-9zWbqH0C_wp/view?usp=drive_link\",\n",
        "    \"https://drive.google.com/file/d/12ee59tcUcxotC1NFfz0EaLNWAAqakKDk/view?usp=drive_link\",\n",
        "    \"https://drive.google.com/file/d/115LBMKIobYRdqKWqL2uR1VWoPS5zqoZq/view?usp=drive_link\",\n",
        "    \"https://drive.google.com/file/d/1CcjaMYUIQNJ2S4nFGHIh0hpkeG158-Ag/view?usp=drive_link\",\n",
        "    \"https://drive.google.com/file/d/1eR6rTQcYw_q4Lob2JFB2_cnND9U2VivA/view?usp=drive_link\",\n",
        "    \"https://drive.google.com/file/d/1Gw9UQGNIcDTLpCaamYm4WoeYqUVlKsAG/view?usp=drive_link\",\n",
        "    \"https://drive.google.com/file/d/1E961JtImN2eis_IxK5EZS3JaqhXkyhK8/view?usp=drive_link\",\n",
        "    \"https://drive.google.com/file/d/1G5xQ10Ijjhrnm_Uq_hl2OgXNaKERfB3G/view?usp=drive_link\",\n",
        "    \"https://drive.google.com/file/d/1u-nmLQIvBdcRomCo__yvoCCVNHV3AiKg/view?usp=drive_link\",\n",
        "    \"https://drive.google.com/file/d/1cfF_cRkvfaTw5BTpiMxalw0Bc-IdBBnO/view?usp=drive_link\",\n",
        "    \"https://drive.google.com/file/d/1WSMqabWY4pElGrQjVVkkaJ8NNEuP6T10/view?usp=drive_link\",\n",
        "    \"https://drive.google.com/file/d/11bCbFObW-51XE0lMS3sz5-VYvWsxctRm/view?usp=drive_link\",\n",
        "    \"https://drive.google.com/file/d/13AFfg8doORRytR3IXfTOmL1vpp6hsCuG/view?usp=drive_link\",\n",
        "    \"https://drive.google.com/file/d/1k8QYuAsyzkMTJA-KzPrOX2BToKmzSr3l/view?usp=drive_link\"\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "hn17StwIHzM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PDF wrangling function and chunk setup\n",
        "\n",
        "def download_google_drive_pdf(share_url, output_folder=\"downloads\"):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    file_id_match = re.search(r\"/d/([^/]+)\", share_url)\n",
        "    if not file_id_match:\n",
        "        raise ValueError(f\"Invalid Google Drive URL: {share_url}\")\n",
        "    file_id = file_id_match.group(1)\n",
        "    download_url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "    response = requests.get(download_url)\n",
        "    pdf_path = os.path.join(output_folder, f\"{file_id}.pdf\")\n",
        "    with open(pdf_path, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "    return pdf_path\n",
        "\n",
        "def extract_text(filepath):\n",
        "    doc = fitz.open(filepath)\n",
        "    return \"\\n\".join(page.get_text(\"text\") for page in doc)\n",
        "\n",
        "def extract_title(text):\n",
        "    candidate_block = text[:1000]\n",
        "    lines = [line.strip() for line in candidate_block.split(\"\\n\") if line.strip()]\n",
        "    for i, line in enumerate(lines):\n",
        "        if line.lower() != line and len(line.split()) > 5 and not line.endswith(\":\") and i < 5:\n",
        "            return line\n",
        "    return \"Unknown Title\"\n",
        "\n",
        "def slugify(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^\\w\\s-]\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \"-\", text)\n",
        "    return text.strip(\"-\")\n",
        "\n",
        "def detect_section(text_chunk, chunk_index=0):\n",
        "    lowered = text_chunk.lower()\n",
        "    if \"introduction\" in lowered[:150] or chunk_index == 0:\n",
        "        return \"Introduction\"\n",
        "    elif \"methods\" in lowered[:150] or \"materials and methods\" in lowered[:150]:\n",
        "        return \"Methods\"\n",
        "    elif \"results\" in lowered[:150]:\n",
        "        return \"Results\"\n",
        "    elif \"discussion\" in lowered[:150]:\n",
        "        return \"Discussion\"\n",
        "    elif \"conclusion\" in lowered[:150]:\n",
        "        return \"Conclusion\"\n",
        "    else:\n",
        "        return \"Unknown\"\n",
        "\n",
        "def chunk_for_weaviate(text, title=None):\n",
        "    from llama_index.core.node_parser import HierarchicalNodeParser\n",
        "    from llama_index.core.text_splitter import SentenceSplitter\n",
        "\n",
        "    if not title:\n",
        "        title = extract_title(text)\n",
        "    slug = slugify(title)\n",
        "    document = Document(text=text, metadata={\"title\": title, \"slug\": slug})\n",
        "\n",
        "    # Create hierarchical parser directly without parameters\n",
        "    # The latest version may not accept parameters in from_defaults()\n",
        "    parser = HierarchicalNodeParser.from_defaults()\n",
        "\n",
        "    # Configure it after creation if needed\n",
        "    # This approach is more compatible with different versions\n",
        "\n",
        "    nodes = parser.get_nodes_from_documents([document])\n",
        "\n",
        "    # Further chunk the nodes if they're too large using SentenceSplitter\n",
        "    text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=50)\n",
        "    smaller_nodes = []\n",
        "    for node in nodes:\n",
        "        split_texts = text_splitter.split_text(node.text)\n",
        "        for i, split_text in enumerate(split_texts):\n",
        "          smaller_nodes.append({\n",
        "              \"text\": split_text,\n",
        "              \"metadata\": {\n",
        "                  \"title\": title,\n",
        "                  \"slug\": slug,\n",
        "                  \"section\": detect_section(split_text, chunk_index=i)\n",
        "                  }\n",
        "              })\n",
        "\n",
        "\n",
        "    return smaller_nodes\n",
        "\n"
      ],
      "metadata": {
        "id": "VqdsmnlxOZ1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunking Function\n",
        "\n",
        "def process_gdrive_links(gdrive_links, output_path=\"demo_chunks.json\"):\n",
        "    all_chunks = []\n",
        "    for link in gdrive_links:\n",
        "        print(f\"Processing: {link}\")\n",
        "        try:\n",
        "            pdf_path = download_google_drive_pdf(link)\n",
        "            text = extract_text(pdf_path)\n",
        "            chunks = chunk_for_weaviate(text)\n",
        "            all_chunks.extend(chunks)\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to process {link}: {e}\")\n",
        "\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(all_chunks, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"\\n✅ Saved {len(all_chunks)} chunks to {output_path}\")"
      ],
      "metadata": {
        "id": "VWsmZt-dV4wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunk PDFs\n",
        "import os\n",
        "process_gdrive_links(gdrive_links, output_path=\"demo_chunks.json\")"
      ],
      "metadata": {
        "id": "z67X_4UnH4xG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to batch upload to Weaviate (including UUIDs)\n",
        "def bulk_upload_space_chunks_to_weaviate(json_file_path, collection_name=\"SpaceMedResearch\"):\n",
        "    import weaviate\n",
        "    from google.colab import userdata\n",
        "    from weaviate.util import generate_uuid5\n",
        "\n",
        "    # Client initialization\n",
        "    WEAVIATE_URL = userdata.get(\"WEAVIATE_URL\")\n",
        "    WEAVIATE_API_KEY = userdata.get(\"WEAVIATE_API_KEY\")\n",
        "    OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "    client = weaviate.connect_to_weaviate_cloud(\n",
        "        WEAVIATE_URL,\n",
        "        auth_credentials=weaviate.AuthApiKey(WEAVIATE_API_KEY),\n",
        "        headers={\n",
        "            \"X-OpenAI-Api-Key\": OPENAI_API_KEY\n",
        "        }\n",
        "    )\n",
        "\n",
        "    docs_collection = client.collections.get(collection_name)\n",
        "\n",
        "    with open(json_file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        chunks = json.load(f)\n",
        "\n",
        "    successful_uploads = 0\n",
        "\n",
        "    with docs_collection.batch.fixed_size(batch_size=100, concurrent_requests=2) as batch:\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            text = chunk.get(\"text\", \"\")\n",
        "            metadata = chunk.get(\"metadata\", {})\n",
        "\n",
        "            # Create a unique ID by combining title with chunk index and first 20 chars of text\n",
        "            unique_id = f\"{metadata.get('title', 'unknown')}-chunk-{i}-{text[:20]}\"\n",
        "            uid = generate_uuid5(unique_id)\n",
        "\n",
        "            batch.add_object(\n",
        "                properties={\n",
        "                    \"content\": text,\n",
        "                    \"title\": metadata.get(\"title\", \"unknown\"),\n",
        "                    \"slug\": metadata.get(\"slug\", \"unknown\"),\n",
        "                    \"section\": metadata.get(\"section\", \"unknown\")\n",
        "                },\n",
        "                uuid=uid\n",
        "            )\n",
        "            successful_uploads += 1\n",
        "\n",
        "            # Progress indicator\n",
        "            if i % 500 == 0 and i > 0:\n",
        "                print(f\"Progress: {i}/{len(chunks)} chunks processed\")\n",
        "\n",
        "            if batch.number_errors > 10:\n",
        "                print(\"❌ Too many errors during batch import — stopping early.\")\n",
        "                break\n",
        "\n",
        "    # Verify the actual count in the collection\n",
        "    collection_count = docs_collection.aggregate.over_all().total_count\n",
        "    print(f\"✅ Uploaded {successful_uploads} chunks to Weaviate from {json_file_path}\")\n",
        "    print(f\"✅ Collection now contains {collection_count} objects\")"
      ],
      "metadata": {
        "id": "uqExyxKhVChx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch upload\n",
        "bulk_upload_space_chunks_to_weaviate(\"/content/demo_chunks.json\")"
      ],
      "metadata": {
        "id": "NA0zn8wyQuqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from weaviate.classes.init import Auth\n",
        "# Try importing from weaviate-agents\n",
        "from weaviate_agents.query import QueryAgent\n",
        "\n",
        "# Instantiate agent object, and specify the collections to query\n",
        "qa = QueryAgent(\n",
        "    client=client, collections=[\"SpaceMedResearch\"]\n",
        ")"
      ],
      "metadata": {
        "id": "_rF_MmEBKlJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform a query\n",
        "response = qa.run(\n",
        "    \"What are the greatest health concerns facing astronauts during their time in space and upon their return to earth?\"\n",
        ")\n",
        "# Print the response\n",
        "response.display()"
      ],
      "metadata": {
        "id": "rISsrg-6KpRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform a query\n",
        "response = qa.run(\n",
        "    \"What are the health concerns for individuals in general aviation?\"\n",
        ")\n",
        "# Print the response\n",
        "response.display()"
      ],
      "metadata": {
        "id": "dODJmr63KwAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform a query\n",
        "response = qa.run(\n",
        "    \"What stress is common in pilots?\"\n",
        ")\n",
        "# Print the response\n",
        "response.display()"
      ],
      "metadata": {
        "id": "aiYSVvljKwyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dgcTFNm-SyvU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}